# ğŸ§  Logistic Regression from Scratch

A clean and minimal implementation of **Logistic Regression** using Python and NumPy â€” built to demonstrate how binary classification works under the hood without relying on external ML libraries.

---

## ğŸ“˜ Overview

Logistic Regression is a **linear model for classification**.  
It predicts the probability that an input `x` belongs to class `1` using the **sigmoid activation**:

`Ïƒ(z) = 1 / (1 + exp(-z))`  
where `z = wáµ€x + b`

---

## âš™ï¸ Mathematical Formulation

### 1. **Hypothesis Function**
`Å· = Ïƒ(wáµ€x + b)`

### 2. **Cost Function (Binary Cross-Entropy)**
`J(w, b) = -(1/m) * Î£ [ y * log(Å·) + (1 - y) * log(1 - Å·) ]`

### 3. **Gradient Descent Updates**
- `dw = (1/m) * Xáµ€(Å· - y)`
- `db = (1/m) * Î£(Å· - y)`
- `w := w - Î± * dw`
- `b := b - Î± * db`

where `Î±` is the **learning rate**.

---

## ğŸ§© Implementation Steps

1. **Data Preprocessing**
   - Normalize input features `X`.
   - Split dataset into train and test sets.

2. **Model Initialization**
   - Initialize parameters `w` and `b` to zeros.

3. **Forward Propagation**
   - Compute `z = wáµ€x + b`
   - Apply `Ïƒ(z)` to get predictions `Å·`.

4. **Compute Cost**
   - Use the binary cross-entropy loss `J(w, b)`.

5. **Backward Propagation**
   - Calculate gradients `dw` and `db`.

6. **Parameter Update**
   - Apply gradient descent using learning rate `Î±`.

7. **Prediction**
   - Predict class `1` if `Å· >= 0.5`, else class `0`.

---

## ğŸ“Š Example Results

After training on a sample dataset:

| Metric | Training | Testing |
|:-------|:----------|:--------|
| Accuracy | 96.2% | 94.8% |
| Cost (final) | 0.081 | 0.097 |

---

## ğŸ§¾ Files Structure

logistic_regression/
â”‚
â”œâ”€â”€ logistic_regression.py # Core implementation
â”œâ”€â”€ dataset.csv # Example dataset
â”œâ”€â”€ README.md # Project documentation
â””â”€â”€ results.png # Training results visualization



---

## ğŸš€ How to Run

```bash
# Clone the repository
git clone https://github.com/yourusername/logistic_regression.git
cd logistic_regression

# Run the model
python logistic_regression.py
ğŸ“ˆ Example Output

Training...
Iteration 1000 | Cost: 0.287
Iteration 2000 | Cost: 0.153
Training complete.

Train Accuracy: 96.2%
Test Accuracy: 94.8%
ğŸ§® Key Concepts
Sigmoid Function: Maps linear values into probabilities.

Cost Function: Measures prediction error.

Gradient Descent: Optimizes weights to minimize cost.

ğŸ’¡ Future Enhancements
Add L2 regularization.

Extend to multiclass classification using one-vs-rest.

Implement a vectorized version for faster computation.

ğŸ§‘â€ğŸ’» Author
Mohamed Maged Elsayed Ahmed Elberry
ğŸ“§ mohamed_berry210@hotmail.com

ğŸ“š References
Andrew Ng â€” Machine Learning (Coursera)

DeepLearning.ai â€” Neural Networks and Deep Learning
