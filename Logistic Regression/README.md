\# ğŸ§® Logistic Regression from Scratch (NumPy Implementation)



A clean and well-documented implementation of \*\*Logistic Regression\*\* using only \*\*NumPy\*\*, built entirely from scratch â€” without using `scikit-learn`â€™s built-in model.



This project demonstrates the core principles behind gradient-based optimization and binary classification using the \*\*sigmoid hypothesis function\*\*.



---



\## ğŸ“˜ Project Overview



This repository contains a minimal and educational implementation of \*\*Logistic Regression\*\*, one of the most fundamental algorithms in Machine Learning.



The goal is to:

\- Understand logistic regression mathematically and programmatically.

\- Implement gradient descent manually.

\- Evaluate model accuracy on a real dataset (Breast Cancer dataset from `sklearn.datasets`).



---



\## ğŸ§  Algorithm Summary



Logistic Regression is a \*\*linear classifier\*\* that models the probability of class membership using the \*\*sigmoid function\*\*:



\\\[

h\_\\theta(x) = \\frac{1}{1 + e^{-(w^T x + b)}}

\\]



We optimize the parameters \\( w \\) and \\( b \\) by minimizing the \*\*logistic loss\*\* (binary cross-entropy) using \*\*gradient descent\*\*:



\\\[

J(w, b) = -\\frac{1}{m} \\sum\_{i=1}^{m} \[y^{(i)} \\log(h\_\\theta(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h\_\\theta(x^{(i)}))]

\\]



---



\## âš™ï¸ Implementation Details



\### ğŸ”¹ `LogesticRegression` class

Defined in `Logestic\_Regression\_Model.py`:



\- `\_\_init\_\_(learning\_rate, n\_iterations)`: Initializes hyperparameters.  

\- `fit(X, y)`: Trains the model using gradient descent.  

\- `hypothesis(X)`: Predicts labels for new samples.  

\- `\_sigmoid(x)`: Applies the sigmoid activation function.



\### ğŸš€ Optimization Logic

Gradients are computed as:



\\\[

\\frac{\\partial J}{\\partial w} = \\frac{1}{m} X^T (h - y), \\quad

\\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum (h - y)

\\]



Parameters are updated iteratively using:



\\\[

w := w - \\alpha \\cdot \\frac{\\partial J}{\\partial w}, \\quad

b := b - \\alpha \\cdot \\frac{\\partial J}{\\partial b}

\\]



---



\## ğŸ§ª Testing \& Evaluation



Testing code is provided in `Logestic\_Regression\_Test.py`.  

The \*\*Breast Cancer dataset\*\* from scikit-learn is used to validate model accuracy.



Example:

```python

regressor = LogesticRegression(learning\_rate=0.001, n\_iterations=1000)

regressor.fit(x\_train, y\_train)

predictions = regressor.hypothesis(x\_test)

print("Logistic Regression accuracy:", accuracy(y\_test, predictions))

âœ… Expected Accuracy: ~0.90â€“0.95 depending on learning rate and iterations.



ğŸ—‚ï¸ Repository Structure

bash

Copy code

machine-learning-models/

â”‚

â”œâ”€â”€ Logestic Regression/

â”‚   â”œâ”€â”€ Logestic\_Regression\_Model.py    # Core model implementation

â”‚   â”œâ”€â”€ Logestic\_Regression\_Test.py     # Dataset loading, training, and evaluation

â”‚   â””â”€â”€ README.md                       # You are here

â”‚

â””â”€â”€ ...

ğŸš€ How to Run

Clone this repository:



bash

Copy code

git clone https://github.com/Berryy2/machine-learning-models.git

cd machine-learning-models/Logestic\\ Regression

Install dependencies:



bash

Copy code

pip install numpy scikit-learn matplotlib

Run the model:



bash

Copy code

python Logestic\_Regression\_Test.py

ğŸ“Š Example Output

yaml

Copy code

Training Logistic Regression...

Model converged after 1000 iterations.

Logistic regression classification accuracy: 0.931578947368421

ğŸ“ˆ Future Improvements

Add Newtonâ€™s Method optimization for faster convergence.



Support multiclass classification via One-vs-Rest strategy.



Add visualization of decision boundaries for 2D datasets.



Compare performance with sklearn.linear\_model.LogisticRegression.



ğŸ’¡ Credits

Developed by Mohamed Maged Elberry

ğŸ“ Masterâ€™s Student â€” Machine Learning \& AI

